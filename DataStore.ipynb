{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ab34c2",
   "metadata": {},
   "source": [
    "# ElasticSearch DataStore Initialization\n",
    "This notebook serves to initialize the DocumentStore and write to it all the saved passages obtained from <i>PDF_Reader</i> and <i>Web_Scraper</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bded3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb801bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path variables\n",
    "\n",
    "file_path = os.getcwd()\n",
    "processed_dir = 'ProcessedData'\n",
    "\n",
    "processed_path = os.path.join(file_path, processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f591a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "#ds_astronomy: document store with data from both papers and web pages\n",
    "ds_astronomy = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"ds_astronomy\"\n",
    ")\n",
    "\n",
    "curr_store = ds_astronomy\n",
    "curr_dir = processed_path\n",
    "localhost = 'http://localhost:9200/ds_astronomy/_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f004850",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Run this cell to get the processed information, both from papers and webpages  ###\n",
    "\n",
    "files_txt = []   #Stores the path to all the TXT files\n",
    "\n",
    "for file in os.listdir(os.path.join(file_path, processed_dir)):\n",
    "    # check only txt files\n",
    "    if file.endswith('.txt'):\n",
    "        files_txt.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30eea47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dark Energy by Robert Caldwell.txt',\n",
       " 'Dark Matter A Primer.txt',\n",
       " 'Geophysical Classification of Planets, Dwarf Planets, and Moons.txt',\n",
       " 'Stars Science Mission Directorate.txt',\n",
       " 'SuperNova Stages.txt',\n",
       " 'Visible Light Science Mission Directorate.txt',\n",
       " 'What Is a Black Hole NASA.txt',\n",
       " 'What makes stars shine.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b162f",
   "metadata": {},
   "source": [
    "<h2>Start ElasticSearch</h2>\n",
    "C:\\Program Files\\Elastic\\Elasticsearch\\7.11.2\\bin\n",
    "\n",
    "Run elasticsearch.exe as Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cedcead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_name': 'elasticsearch',\n",
       " 'status': 'yellow',\n",
       " 'timed_out': False,\n",
       " 'number_of_nodes': 1,\n",
       " 'number_of_data_nodes': 1,\n",
       " 'active_primary_shards': 5,\n",
       " 'active_shards': 5,\n",
       " 'relocating_shards': 0,\n",
       " 'initializing_shards': 0,\n",
       " 'unassigned_shards': 5,\n",
       " 'delayed_unassigned_shards': 0,\n",
       " 'number_of_pending_tasks': 0,\n",
       " 'number_of_in_flight_fetch': 0,\n",
       " 'task_max_waiting_in_queue_millis': 0,\n",
       " 'active_shards_percent_as_number': 50.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell to ensure that ElasticSearch is running\n",
    "requests.get('http://localhost:9200/_cluster/health').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b13ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellow open scrap_astronomy m2wGWiloRf2WBv580_DBig 1 1  12 0 203.7kb 203.7kb\\nyellow open pdf_astronomy   q5fonswfTv-B32EH89kaGw 1 1 263 0   3.9mb   3.9mb\\nyellow open label           QpdbKOnESnanAYyOa1pwKQ 1 1   0 0    208b    208b\\nyellow open astronomy       6ytjQkX2RbGTDV0cp-VGuA 1 1   0 0    208b    208b\\nyellow open ds_astronomy    kl-xrSTTT2OCr8IMEO75MA 1 1 168 0   2.6mb   2.6mb\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://localhost:9200/_cat/indices').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1abe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tot_passages(data_list, file_names):\n",
    "    for idx, inst in enumerate(data_list):\n",
    "        words = 0\n",
    "        for passage in inst:\n",
    "            words += len(passage.split())\n",
    "        print(f'{file_names[idx]} has {len(inst)} passages, with an average of {words/len(inst)} words per passage!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc169388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.empty(len(files_txt), dtype=object) #data[document][paragraph]\n",
    "\n",
    "for file_idx in range(len(files_txt)):\n",
    "    #if files_txt[file_idx] == 'Dark Energy by Robert Caldwell.txt':\n",
    "    #print(os.path.join(curr_dir, files_txt[file_idx]))\n",
    "    with open(os.path.join(curr_dir, files_txt[file_idx]), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        text_split = text.split('\\n')\n",
    "\n",
    "        if len(text_split) == 1:\n",
    "            text = text.split('\\\\r\\\\n')\n",
    "        else:\n",
    "            text = text_split\n",
    "        if text[-1] == '':\n",
    "            data[file_idx] = text[0:-1]\n",
    "        else:\n",
    "            data[file_idx] = text\n",
    "\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db2ecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark Energy by Robert Caldwell.txt has 12 passages, with an average of 489.9166666666667 words per passage!\n",
      "Dark Matter A Primer.txt has 111 passages, with an average of 102.44144144144144 words per passage!\n",
      "Geophysical Classification of Planets, Dwarf Planets, and Moons.txt has 18 passages, with an average of 496.3888888888889 words per passage!\n",
      "Stars Science Mission Directorate.txt has 10 passages, with an average of 80.1 words per passage!\n",
      "SuperNova Stages.txt has 1 passages, with an average of 175.0 words per passage!\n",
      "Visible Light Science Mission Directorate.txt has 17 passages, with an average of 39.05882352941177 words per passage!\n",
      "What Is a Black Hole NASA.txt has 21 passages, with an average of 29.952380952380953 words per passage!\n",
      "What makes stars shine.txt has 14 passages, with an average of 62.214285714285715 words per passage!\n"
     ]
    }
   ],
   "source": [
    "get_tot_passages(data , files_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = np.empty(len(data), dtype=object) #data[document][data]\n",
    "\n",
    "for file_idx in range(len(files_txt)):\n",
    "    title = files_txt[file_idx].split('.txt')[0]\n",
    "    data_json[file_idx] = [\n",
    "        {\n",
    "            'content': paragraph,\n",
    "            'meta': {\n",
    "                'source': title\n",
    "            }\n",
    "        } for paragraph in data[file_idx]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b3578",
   "metadata": {},
   "source": [
    "Write passages to ElasticSearch DocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c5df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to empty document store\n",
    "curr_store.delete_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c71dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_idx in range(len(data_json)):\n",
    "    curr_store.write_documents(data_json[json_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e06a26",
   "metadata": {},
   "source": [
    "Run bottom cell to get the currently stored passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63a6c7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 200,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'count': shows how many passages are present in the DataStore\n",
    "requests.get(localhost).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f314c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c28207cbf101b5fd31f062ed72814c7d2a1b366155535ea78ffa95e3f97ec6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
