{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49db1ca8",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "\n",
    "Runing this notebook enables the scraping of saved web-pages inside the <b>WebPages</b> folder. Each present HTML file is scraped and saved respectively in a TXT file inside the <b>ScrapedHTML</b> folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d9144",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb46e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import urllib\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ca9dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining path variables\n",
    "\n",
    "file_path = os.getcwd()\n",
    "\n",
    "processed_html = 'ProcessedData'\n",
    "raw_html = 'WebPages'\n",
    "\n",
    "scraped_path = os.path.join(file_path, processed_html)\n",
    "html_path = os.path.join(file_path, raw_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9647b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unprocessed HTML files\n",
    "\n",
    "web_pages = []      #Saves the html name of each file\n",
    "web_html_path = []  #Saves the full path of all the html files\n",
    "\n",
    "for file in os.listdir(os.path.join(file_path, raw_html)):\n",
    "    # check only html files\n",
    "    if file.endswith('.html'):\n",
    "        web_html_path.append(os.path.join(html_path, file))\n",
    "        web_pages.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504593f4",
   "metadata": {},
   "source": [
    "Each passage/paragraph is pre-processed before being saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bf57045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_paragraph(paragraph):\n",
    "    ponctuation = \"#$()*+-/:;<=>@[\\]^_`{|}\"\n",
    "    special_characters = 'å¼«¥ª°©ð±§µæ¹¢³¿®ä£⊙'\n",
    "    email = '[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}'\n",
    "    itemized = '[(\\s][0-9a-zA-Z][.)]\\s+|[(\\s][ivxIVX]+[.)]\\s+'\n",
    "    \n",
    "    processed_text = re.sub(\"\\<a.+?\\</a>\", \"\", paragraph)\n",
    "    processed_text = re.sub(\"\\[.*?\\]\", \"\", processed_text)                                  \n",
    "    processed_text = re.sub(\"\\<b>.+?\\</b>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"\\<i>.+?\\</i>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"\\<strong>.+?\\</strong>\", \"\", processed_text)\n",
    "    processed_text = processed_text.translate(str.maketrans('', '', special_characters))\n",
    "    processed_text = re.sub(\"\\<em>.+?\\</em>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"</b>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"<br/>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"<p>\", \"\", processed_text)\n",
    "    processed_text = re.sub(\"</p>\", \"\", processed_text)\n",
    "    #processed_text = re.sub(\"\\n\", \"\", processed_text)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1bfb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_title(title):\n",
    "    ponctuation = \"#$()*+-/:;<=>@[\\]^_`{|}?!\"\n",
    "    special_characters = 'å¼«¥ª°©ð±§µæ¹¢³¿®ä£⊙'\n",
    "    \n",
    "    processed_title = title.translate(str.maketrans('', '', ponctuation))        #Removes some ponctuation\n",
    "    processed_title = processed_title.translate(str.maketrans('', '', special_characters)) #Removes special characters\n",
    "    processed_title = ' '.join(re.split('\\s+', processed_title.strip(), flags=re.UNICODE)) #Removes duplicated spaces\n",
    "    \n",
    "    return processed_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac3ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars Science Mission Directorate - was saved successfully\n",
      "What Is a Black Hole NASA - was saved successfully\n"
     ]
    }
   ],
   "source": [
    "for web_page in web_html_path:\n",
    "    with open(web_page, encoding=\"utf8\") as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'lxml')\n",
    "    text = soup.find_all('p')\n",
    "    title = soup.find('title').string\n",
    "    title = pre_process_title(title)\n",
    "    html_file.close()\n",
    "    processed_content = []\n",
    "\n",
    "    for p in text:\n",
    "        #print(p.string)\n",
    "        content = pre_process_paragraph(str(p))\n",
    "        if content.count(' ') > 4:\n",
    "            processed_content.append(content)\n",
    "    #write content to txt\n",
    "    with open(os.path.join(scraped_path, title)+'.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for p in processed_content:\n",
    "            f.write(p)\n",
    "    f.close()\n",
    "    \n",
    "    print(f'{title} - was saved successfully')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299030bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
